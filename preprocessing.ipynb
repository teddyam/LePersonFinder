{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LePersonFinder\n",
        "Description: looks for people. \n",
        "Notes: The following code is for colab specific stuff, but see the below for non-colab specific paths. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88ZgqwNVqfjE",
        "outputId": "b5553a87-ea72-4eb9-fb7a-68368c3628bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "isColab = \"google.colab\" in sys.modules\n",
        "\n",
        "if isColab:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "    student_colab_path = (\"/content/drive/MyDrive\"\n",
        "        + \"/Colab Notebooks\")\n",
        "    sys.path.append(student_colab_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrEKlNJIx6GR",
        "outputId": "8db60a13-f4ae-498b-ab82-f41644797e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WMf3nAPYyd0R"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Rescaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Aer8RX8SyNIx"
      },
      "outputs": [],
      "source": [
        "## COLAB SPECIFIC PATHS \n",
        "data_folder_path = \"/content/drive/MyDrive/Colab Notebooks/heridal_keras_retinanet_voc\"\n",
        "images_path = data_folder_path + \"/JPEGImages\"\n",
        "annotations_path = data_folder_path + \"/Annotations/JSON\"\n",
        "ImageSets = data_folder_path + \"/ImageSets/Main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "## GITHUB PATHS \n",
        "data_folder_path = \"data/heridal_keras_retinanet_voc\"\n",
        "images_path = data_folder_path + \"/JPEGImages\"\n",
        "annotations_path = data_folder_path + \"/Annotations/JSON\"\n",
        "ImageSets = data_folder_path + \"/ImageSets/Main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yCcLFUGJBLwN"
      },
      "outputs": [],
      "source": [
        "def xml_to_json(xml_folder, json_folder):\n",
        "    os.makedirs(json_folder, exist_ok=True)\n",
        "\n",
        "    for xml_file in os.listdir(xml_folder):\n",
        "        if xml_file.endswith('.xml'):\n",
        "            xml_path = os.path.join(xml_folder, xml_file)\n",
        "            json_path = os.path.join(json_folder, xml_file.replace('.xml', '.json'))\n",
        "\n",
        "            tree = ET.parse(xml_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            data_dict = {\n",
        "                'folder': root.find('folder').text if root.find('folder') is not None else 'Unknown',\n",
        "                'filename': root.find('filename').text if root.find('filename') is not None else 'Unknown',\n",
        "                'size': {\n",
        "                    'width': int(root.find('size/width').text) if root.find('size/width') is not None else 0,\n",
        "                    'height': int(root.find('size/height').text) if root.find('size/height') is not None else 0,\n",
        "                    'depth': int(root.find('size/depth').text) if root.find('size/depth') is not None else 0,\n",
        "                },\n",
        "                'objects': []\n",
        "            }\n",
        "\n",
        "            for obj in root.findall('object'):\n",
        "                bbox = obj.find('bndbox')\n",
        "                data_dict['objects'].append({\n",
        "                    'name': obj.find('name').text if obj.find('name') is not None else 'Unknown',\n",
        "                    'pose': obj.find('pose').text if obj.find('pose') is not None else 'Unspecified',\n",
        "                    'truncated': int(obj.find('truncated').text) if obj.find('truncated') is not None else 0,\n",
        "                    'difficult': int(obj.find('difficult').text) if obj.find('difficult') is not None else 0,\n",
        "                    'bbox': {\n",
        "                        'xmin': int(bbox.find('xmin').text) if bbox.find('xmin') is not None else 0,\n",
        "                        'ymin': int(bbox.find('ymin').text) if bbox.find('ymin') is not None else 0,\n",
        "                        'xmax': int(bbox.find('xmax').text) if bbox.find('xmax') is not None else 0,\n",
        "                        'ymax': int(bbox.find('ymax').text) if bbox.find('ymax') is not None else 0,\n",
        "                    }\n",
        "                })\n",
        "\n",
        "            with open(json_path, 'w') as json_file:\n",
        "                json.dump(data_dict, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mDer5edLs6AV"
      },
      "outputs": [],
      "source": [
        "#only run if json files don't exist yet\n",
        "xml_folder = data_folder_path + \"/Annotations\"\n",
        "json_folder = annotations_path\n",
        "xml_to_json(xml_folder, json_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A27j8oOBEvdW"
      },
      "outputs": [],
      "source": [
        "def parse_json(json_string):\n",
        "    data = json.loads(json_string.decode('utf-8'))\n",
        "    if 'objects' not in data or not data['objects']:\n",
        "        return np.zeros((0, 4), dtype=np.float32), np.int32(data['size']['width']), np.int32(data['size']['height'])\n",
        "    bounding_boxes = np.array([\n",
        "        [np.int32(obj['bbox']['xmin']), np.int32(obj['bbox']['ymin']),\n",
        "         np.int32(obj['bbox']['xmax']), np.int32(obj['bbox']['ymax'])]\n",
        "        for obj in data['objects'] if obj['name'] == 'person'\n",
        "    ], dtype=np.float32)\n",
        "    original_width = np.int32(data['size']['width'])\n",
        "    original_height = np.int32(data['size']['height'])\n",
        "    return bounding_boxes, original_width, original_height\n",
        "\n",
        "def load_json(annotation_path):\n",
        "    json_string = tf.io.read_file(annotation_path)\n",
        "    bounding_boxes, original_width, original_height = tf.numpy_function(\n",
        "        parse_json, [json_string], [tf.float32, tf.int32, tf.int32]\n",
        "    )\n",
        "    bounding_boxes.set_shape([None, 4])\n",
        "    original_width.set_shape([])\n",
        "    original_height.set_shape([])\n",
        "    return bounding_boxes, original_width, original_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iVqLCizzy4KA"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 256\n",
        "PATCH_SIZE = 16\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZjUWk6gr18BA"
      },
      "outputs": [],
      "source": [
        "def scale_bounding_boxes(bounding_boxes, original_width, original_height, new_width, new_height, pad_w, pad_h):\n",
        "  if tf.size(bounding_boxes) == 0:\n",
        "    return tf.zeros((0, 4), dtype=tf.float32)\n",
        "\n",
        "  original_width = tf.cast(original_width, tf.float32)\n",
        "  original_height = tf.cast(original_height, tf.float32)\n",
        "  new_width = tf.cast(new_width, tf.float32)\n",
        "  new_height = tf.cast(new_height, tf.float32)\n",
        "  pad_w = tf.cast(pad_w[0], tf.float32)\n",
        "  pad_h = tf.cast(pad_h[0], tf.float32)\n",
        "\n",
        "  scale_x = new_width / original_width\n",
        "  scale_y = new_height / original_height\n",
        "\n",
        "  xmin_scaled = bounding_boxes[:, 0] * scale_x + pad_w\n",
        "  ymin_scaled = bounding_boxes[:, 1] * scale_y + pad_h\n",
        "  xmax_scaled = bounding_boxes[:, 2] * scale_x + pad_w\n",
        "  ymax_scaled = bounding_boxes[:, 3] * scale_y + pad_h\n",
        "\n",
        "  bounding_boxes_scaled = tf.stack([xmin_scaled, ymin_scaled, xmax_scaled, ymax_scaled], axis=1)\n",
        "\n",
        "  return bounding_boxes_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yKetMqwSqZNs"
      },
      "outputs": [],
      "source": [
        "def compute_overlap_counts(bboxes, image_size, patch_size):\n",
        "    num_patches_side = image_size // patch_size\n",
        "    patch_indices = tf.range(0, image_size, patch_size)\n",
        "    start_indices = tf.cast(tf.stack(tf.meshgrid(patch_indices, patch_indices, indexing='ij'), axis=-1), tf.float32)\n",
        "    end_indices = start_indices + patch_size\n",
        "\n",
        "    overlap_counts = tf.zeros((num_patches_side, num_patches_side), dtype=tf.int32)\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        xmin, ymin, xmax, ymax = bbox[0], bbox[1], bbox[2], bbox[3]\n",
        "        overlaps_x = tf.logical_and(start_indices[..., 1] < xmax, start_indices[..., 1] + patch_size > xmin)\n",
        "        overlaps_y = tf.logical_and(start_indices[..., 0] < ymax, start_indices[..., 0] + patch_size > ymin)\n",
        "        overlaps = tf.logical_and(overlaps_x, overlaps_y)\n",
        "\n",
        "        overlap_counts += tf.cast(overlaps, tf.int32)\n",
        "    \n",
        "    return tf.expand_dims(overlap_counts, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uK3sn44E2t41"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, annotation_path):\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "  original_shape = tf.cast(tf.shape(image)[:2], tf.float32)\n",
        "  ratio = IMAGE_SIZE / tf.reduce_max(original_shape)\n",
        "  new_shape = tf.round(original_shape * ratio)\n",
        "\n",
        "  image = tf.image.resize(image, tf.cast(new_shape, tf.int32))\n",
        "\n",
        "  pad_w = (IMAGE_SIZE - new_shape[1]) / 2\n",
        "  pad_h = (IMAGE_SIZE - new_shape[0]) / 2\n",
        "  pad_w = [pad_w, IMAGE_SIZE - new_shape[1] - pad_w]\n",
        "  pad_h = [pad_h, IMAGE_SIZE - new_shape[0] - pad_h]\n",
        "\n",
        "  image = tf.pad(image, [[int(pad_h[0]), int(pad_h[1])], [int(pad_w[0]), int(pad_w[1])], [0, 0]], constant_values=0)\n",
        "  image = Rescaling(1./255)(image)\n",
        "\n",
        "  patches = tf.image.extract_patches(\n",
        "      images=tf.expand_dims(image, 0),\n",
        "      sizes=[1, PATCH_SIZE, PATCH_SIZE, 1],\n",
        "      strides=[1, PATCH_SIZE, PATCH_SIZE, 1],\n",
        "      rates=[1, 1, 1, 1],\n",
        "      padding='VALID'\n",
        "  )\n",
        "  patches = tf.reshape(patches, [-1, PATCH_SIZE * PATCH_SIZE * 3])\n",
        "\n",
        "  bounding_boxes, original_width, original_height = load_json(annotation_path)\n",
        "  bounding_boxes_scaled = scale_bounding_boxes(\n",
        "      bounding_boxes, original_width, original_height,\n",
        "      new_shape[1], new_shape[0], pad_w, pad_h\n",
        "  )\n",
        "\n",
        "  overlap_counts = compute_overlap_counts(bounding_boxes_scaled, IMAGE_SIZE, PATCH_SIZE)\n",
        "\n",
        "  return patches, overlap_counts, bounding_boxes_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "n2Ha09g-6HFy"
      },
      "outputs": [],
      "source": [
        "def file_exists(file_path):\n",
        "    return tf.io.gfile.exists(file_path.numpy().decode())\n",
        "\n",
        "def create_filtered_dataset(images_path, annotations_path, subset_prefix):\n",
        "    image_files = tf.data.Dataset.list_files(os.path.join(images_path, subset_prefix + '_*.jpg'))\n",
        "\n",
        "    def filter_func(image_file):\n",
        "        annotation_file = tf.strings.regex_replace(image_file, 'JPEGImages', 'Annotations')\n",
        "        annotation_file = tf.strings.regex_replace(annotation_file, '\\.jpg', '.xml')\n",
        "        return tf.py_function(file_exists, [annotation_file], Tout=tf.bool)\n",
        "\n",
        "    filtered_image_dataset = image_files.filter(filter_func)\n",
        "\n",
        "    def load_and_preprocess_image(image_file):\n",
        "        annotation_file = tf.strings.regex_replace(image_file, 'JPEGImages', 'Annotations/JSON')\n",
        "        annotation_file = tf.strings.regex_replace(annotation_file, '\\.jpg', '.json')\n",
        "        return preprocess_image(image_file, annotation_file)\n",
        "\n",
        "    dataset = filtered_image_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bt2L-eN-acb",
        "outputId": "8a25c7b4-d680-46f2-d5c3-2417f5952d98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-20 11:36:21.995090: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-20 11:36:21.995524: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train patches shape: (256, 768)\n",
            "Train bounding boxes: tf.Tensor(\n",
            "[[222.848    108.22401  226.62401  113.600006]\n",
            " [ 23.04     143.42401   26.560001 146.176   ]\n",
            " [170.11201  212.92801  173.184    217.53601 ]], shape=(3, 4), dtype=float32)\n",
            "Train overlap counts shape: (16, 16, 1)\n",
            "Train sample overlap counts: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-20 11:36:22.617993: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [101]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-20 11:36:22.618230: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [101]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test patches shape: (256, 768)\n",
            "Test bounding boxes: tf.Tensor(\n",
            "[[119.104004 140.09601  121.79201  145.728   ]\n",
            " [121.66401  141.632    125.12     147.58401 ]\n",
            " [125.184006 144.384    127.04001  147.45601 ]], shape=(3, 4), dtype=float32)\n",
            "Test overlap counts shape: (16, 16, 1)\n",
            "Test sample overlap counts: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = create_filtered_dataset(images_path, annotations_path, 'train')\n",
        "test_dataset = create_filtered_dataset(images_path, annotations_path, 'test')\n",
        "\n",
        "for patches, overlap_counts, bboxes in train_dataset.take(1):\n",
        "    print('Train patches shape:', patches.shape)\n",
        "    print('Train bounding boxes:', bboxes)\n",
        "    print('Train overlap counts shape:', overlap_counts.shape)\n",
        "    print('Train sample overlap counts:', overlap_counts.numpy().flatten()[:256])\n",
        "\n",
        "for patches, overlap_counts, bboxes in test_dataset.take(1):\n",
        "    print('Test patches shape:', patches.shape)\n",
        "    print('Test bounding boxes:', bboxes)\n",
        "    print('Test overlap counts shape:', overlap_counts.shape)\n",
        "    print('Test sample overlap counts:', overlap_counts.numpy().flatten()[:256])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "XaoP6ksmxPRu",
        "outputId": "3f995925-044c-4505-dbe5-f8fb672832ec"
      },
      "outputs": [],
      "source": [
        "def display_image_with_bboxes(image_patches, overlap_counts, bbox_data, num_patches_side):\n",
        "    patches_reshaped = tf.reshape(image_patches, [num_patches_side, num_patches_side, PATCH_SIZE, PATCH_SIZE, 3])\n",
        "    patches_transposed = tf.transpose(patches_reshaped, [0, 2, 1, 3, 4])\n",
        "    image_reconstructed = tf.reshape(patches_transposed, [IMAGE_SIZE, IMAGE_SIZE, 3])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.imshow(image_reconstructed)\n",
        "\n",
        "    for bbox in bbox_data:\n",
        "        y_min = bbox[1]\n",
        "        x_min = bbox[0]\n",
        "        y_max = bbox[3]\n",
        "        x_max = bbox[2]\n",
        "        print(f\"Bounding Box: [{x_min}, {y_min}, {x_max}, {y_max}]\")\n",
        "\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        rect = mpatches.Rectangle((x_min, y_min), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    for i in range(num_patches_side):\n",
        "        for j in range(num_patches_side):\n",
        "            if overlap_counts[i, j, 0] > 0:\n",
        "                patch_x = j * PATCH_SIZE\n",
        "                patch_y = i * PATCH_SIZE\n",
        "                rect = mpatches.Rectangle((patch_x, patch_y), PATCH_SIZE, PATCH_SIZE, linewidth=1, edgecolor='yellow', facecolor='none', linestyle='--')\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(patch_x + PATCH_SIZE / 2, patch_y + PATCH_SIZE / 2, str(overlap_counts[i, j, 0]),\n",
        "                        color='yellow', ha='center', va='center', fontsize=8)\n",
        "\n",
        "    plt.title(\"Reconstructed Image with Bounding Boxes and Highlight Patches\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for image_patches, overlap_counts, bbox_data in train_dataset.take(1):\n",
        "    num_patches_side = IMAGE_SIZE // PATCH_SIZE\n",
        "    display_image_with_bboxes(image_patches.numpy(), overlap_counts.numpy(), bbox_data.numpy(), num_patches_side)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Class\n",
        "The following code is for the Model class of the Transformer model. \n",
        "\n",
        "Some key notes about our dataset: \n",
        "- 7 classes \n",
        "- (256, 768) inputs\n",
        "\n",
        "Some key notes about other parameters: \n",
        "- Query: of shape (batch_sz, target_seq_length, feature_dim)\n",
        "- Value: of shape (batch_sz, source_seq_length, feature_dim)\n",
        "- Key: of shape (batch_sz, source_seq_length, feature_dim)\n",
        "\n",
        "Our architecture using the below classes goes as follows: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters/Stats\n",
        "Hyperparameters and stats for our model pre-compilation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the total number of classes:  {0, 1, 2, 3, 4, 5, 6, 7}\n",
            "This is the size of the train dataset:  1546\n",
            "These are the img patches shape:  {TensorShape([256, 768])}\n",
            "These are overlap cts shape:  {TensorShape([16, 16, 1])}\n",
            "These are bbox shape:  {TensorShape([12, 4]), TensorShape([3, 4]), TensorShape([14, 4]), TensorShape([5, 4]), TensorShape([16, 4]), TensorShape([7, 4]), TensorShape([29, 4]), TensorShape([18, 4]), TensorShape([20, 4]), TensorShape([22, 4]), TensorShape([11, 4]), TensorShape([33, 4]), TensorShape([9, 4]), TensorShape([2, 4]), TensorShape([35, 4]), TensorShape([0, 4]), TensorShape([13, 4]), TensorShape([24, 4]), TensorShape([15, 4]), TensorShape([26, 4]), TensorShape([6, 4]), TensorShape([17, 4]), TensorShape([4, 4]), TensorShape([28, 4]), TensorShape([8, 4]), TensorShape([19, 4]), TensorShape([10, 4]), TensorShape([1, 4]), TensorShape([21, 4])}\n",
            "This is the size of the test dataset:  101\n"
          ]
        }
      ],
      "source": [
        "## Get all uq counts (this is super inefficient: 30 sec long, someone please understand Sam's code so vectorize BUT this does work)\n",
        "all_cts = set()\n",
        "all_cts.add(0)\n",
        "img_patches_shape = set()\n",
        "overlap_cts_shape = set()\n",
        "bbox_data_shape = set() \n",
        "len_train = 0\n",
        "for image_patches, overlap_counts, bbox_data in train_dataset:\n",
        "    len_train += 1  \n",
        "    uq_ct = np.unique(overlap_counts.numpy())\n",
        "    uq_ct = uq_ct[1:] \n",
        "    img_patches_shape.add(image_patches.shape)\n",
        "    overlap_cts_shape.add(overlap_counts.shape)\n",
        "    bbox_data_shape.add(bbox_data.shape) ## Bbox shape is NOT uniform unfortunately \n",
        "    for el in uq_ct: \n",
        "        all_cts.add(el)\n",
        "print(\"This is the total number of classes: \", all_cts)\n",
        "print(\"This is the size of the train dataset: \", len_train)\n",
        "print(\"These are the img patches shape: \", img_patches_shape)\n",
        "print(\"These are overlap cts shape: \", overlap_cts_shape)\n",
        "print(\"These are bbox shape: \", bbox_data_shape)\n",
        "\n",
        "len_test = 0\n",
        "for image_patches, overlap_counts, bbox_data in test_dataset: \n",
        "    len_test += 1\n",
        "print(\"This is the size of the test dataset: \", len_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Hyperparameters & general stats\n",
        "params = {\n",
        "    'batch_sz': 64, \n",
        "    'num_classes': 8,\n",
        "    'vocab_sz': 8,  \n",
        "    'num_att_heads': 1, \n",
        "    'key_dim': 10, \n",
        "    'query_dim': 10, \n",
        "    'value_dim': 10, \n",
        "    'output_sz': 8, \n",
        "    'window_sz': 256,\n",
        "    'num_features': 768, \n",
        "    'emb_sz': 132, \n",
        "    'num_layers': 2, \n",
        "    'learning_rate': 0.01, \n",
        "    'num_epochs': 20, \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional Encoding \n",
        "This defines our positional encoding class. This will positionally encode our input tensor so that the Transformer has some understanding of how the pixels are distributed spatially. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the positional embedding shape:  (64, 256, 132)\n"
          ]
        }
      ],
      "source": [
        "## Taken from class - positional encoding method. \n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1) \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "## Taken from class - positional embedding class that will look a token's embedding vector and add the corresponding position vector \n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, emb_sz):\n",
        "    super().__init__()\n",
        "    self.emb_sz = emb_sz\n",
        "    # self.embedding = tf.keras.layers.Embedding(vocab_size, emb_sz, mask_zero=True) <- embedding for NLP-like tasks \n",
        "    self.embedding = tf.keras.layers.Dense(emb_sz)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=emb_sz)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.emb_sz, tf.float32)) # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "  \n",
        "## UNIT TEST: check sizes @ embedding \n",
        "test_emb = tf.zeros((params['batch_sz'], params['window_sz'], params['num_features']))\n",
        "embed_img = PositionalEmbedding(vocab_size=params['vocab_sz'], emb_sz=params['emb_sz'])\n",
        "embed = embed_img(test_emb)\n",
        "print(\"This is the positional embedding shape: \", embed.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention\n",
        "- BaseAttention: super class that other variations of Attention can inherit from. Essentially combines Attention + LayerNormalization into one class.\n",
        "- Other subclasses..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the shape post-self attention:  (64, 256, 132)\n"
          ]
        }
      ],
      "source": [
        "## Base Attention Class\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "## This class is just a basic implementation of self-attention \n",
        "class GlobalSelfAttention(BaseAttention): \n",
        "    def call(self, x): \n",
        "      attn_output, attn_scores = self.mha(query=x, key=x, value=x, return_attention_scores=True)\n",
        "      # Cache the attention scores for plotting later.\n",
        "      self.last_attn_scores = attn_scores\n",
        "      x = self.add([x, attn_output])\n",
        "      x = self.layernorm(x)\n",
        "      return x\n",
        "    \n",
        "## This class is just the masked implementation of self-attention\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "## This class connects the Encoder and Decoder: takes in context sequence for keys and values as opposed to the input sequence.  \n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    # Multi-headed attention \n",
        "    attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "  \n",
        "sample_sa = GlobalSelfAttention(num_heads=params['num_att_heads'], key_dim=params['key_dim'])\n",
        "att_on_emb = sample_sa(embed)\n",
        "print(\"This is the shape post-self attention: \", att_on_emb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FeedForward Layers\n",
        "- This class defines feedforward layers that produce our desired output after we apply attention to our input. This combines ADD + LayerNormalization in as well. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, emb_sz, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(emb_sz),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder Block\n",
        "There are two classes in this section: \n",
        "- Encoder Layer: one unit of the block. Attention -> LayerNorm -> FeedForward NN. \n",
        "- Encoder Block: we can have many stacked Encoder layers. All this does is create a positional embedding for the input, add it to the input, then pass that into a stack of Encoder layers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, emb_sz, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.self_attention = BaseAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=emb_sz,\n",
        "        dropout=dropout_rate)\n",
        "    self.ffn = FeedForward(emb_sz, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, emb_sz, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.emb_sz = emb_sz\n",
        "    self.num_layers = num_layers\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, emb_sz=emb_sz)\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(emb_sz=emb_sz,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Should result in shape of (batch_size, seq_len, emb_sz).\n",
        "    x = self.dropout(x)  # Add dropout.\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "    return x  # Should result in shape of (batch_size, seq_len, emb_sz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decoder Block \n",
        "There are two classes in this section: \n",
        "- Decoder Block: one unit of the block. Self attention -> Cross attention -> Feed forward network. \n",
        "- Decoder: stacked Decoder layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, emb_sz, num_heads, dff, dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    ## Basic self-attention (tutorials might use causal self attention, look into what that does)\n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=emb_sz, dropout=dropout_rate)\n",
        "    ## Encoder-decoder attention \n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=emb_sz,\n",
        "        dropout=dropout_rate)\n",
        "    self.ffn = FeedForward(emb_sz, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, emb_sz)`.\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, emb_sz, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.emb_sz = emb_sz\n",
        "    self.num_layers = num_layers\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, emb_sz=emb_sz)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [DecoderLayer(emb_sz=emb_sz, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, emb_sz)\n",
        "    x = self.dropout(x)\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "    return x # The shape of x is (batch_size, target_seq_len, emb_sz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Model\n",
        "This combines our Encoder and Decoder blocks into a coherent model. It should look something like the following: \n",
        "- Positional Encoding + Addition of Context => \n",
        "- => Encoder Block: \n",
        "    - Encoder Layer 1: \n",
        "        - Self Attention \n",
        "        - Feed Forward NN \n",
        "            - Dense 1 \n",
        "            - Dense 2 \n",
        "            - Dropout \n",
        "            - Add & LayerNormalization\n",
        "    - Encoder Layer 2: \n",
        "        - Self Attention \n",
        "        - Feed Forward NN \n",
        "            - Dense 1 \n",
        "            - Dense 2 \n",
        "            - Dropout \n",
        "            - Add & LayerNormalization \n",
        "    - etc ...\n",
        "- => Decoder Block: \n",
        "    - Decoder Layer 1: \n",
        "        - Causal Self Attention \n",
        "        - Cross Attention \n",
        "        - Feed Forward NN \n",
        "            - Dense 1\n",
        "            - Dense 2 \n",
        "            - Dropout \n",
        "            - Add & LayerNormalization\n",
        "    - etc ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, emb_sz, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, emb_sz=emb_sz,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, emb_sz=emb_sz,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  ## FORWARD PASS\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the first argument.\n",
        "    context, x  = inputs\n",
        "    # The following should return: (batch_sz, 256, emb_sz)\n",
        "    context = self.encoder(context) # ** (batch_size, context_len, emb_sz)\n",
        "    # The following should return: (batch_size, 256, emb_sz)\n",
        "    x = self.decoder(x, context)  # ** (batch_size, target_len, emb_sz)\n",
        "    # The following should return: (batch_size, 256, 8)\n",
        "    logits = self.final_layer(x)  # ** (batch_size, target_len, target_vocab_size)\n",
        "    # nk - IDK what this following section does...\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics. b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits\n",
        "  \n",
        "  ## TRAIN STEP (1 per epoch)\n",
        "  def train(): \n",
        "    pass "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy and Loss Metrics\n",
        "Title. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "  mask = label != 0\n",
        "  match = match & mask\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and Fit the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-20 18:12:47.895501: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-20 18:12:47.895769: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [2,4], [batch]: [13,4] [Op:IteratorGetNext]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[98], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m len_train \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_sz\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mshuffle(buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# shuffle the dataset every epoch\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batched_train_dataset: \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:797\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    796\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:780\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 780\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3014\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3016\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3018\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7261\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7262\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [2,4], [batch]: [13,4] [Op:IteratorGetNext]"
          ]
        }
      ],
      "source": [
        "## Instantiations of above objects\n",
        "transformer = Transformer(num_layers=params['num_layers'], \n",
        "                          emb_sz=params['emb_sz'],\n",
        "                          num_heads=params['num_att_heads'], \n",
        "                          dff=params['num_features'], \n",
        "                          input_vocab_size=params['vocab_sz'],\n",
        "                          target_vocab_size=params['vocab_sz'])\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
        "\n",
        "## Train batches\n",
        "\n",
        "## Compile the model \n",
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])\n",
        "\n",
        "## Custom train model method  \n",
        "batched_train_dataset = train_dataset.batch(batch_size=params['batch_sz'], drop_remainder=False)\n",
        "for e in range(params['num_epochs']): \n",
        "    num_batches = len_train // params['batch_sz']\n",
        "    train_dataset.shuffle(buffer_size=3) # shuffle the dataset every epoch\n",
        "    for batch in batched_train_dataset: \n",
        "        print(batch)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
