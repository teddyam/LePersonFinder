{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LePersonFinder\n",
        "Description: looks for people. \n",
        "Notes: The following code is for colab specific stuff, but see the below for non-colab specific paths. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88ZgqwNVqfjE",
        "outputId": "b5553a87-ea72-4eb9-fb7a-68368c3628bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "isColab = \"google.colab\" in sys.modules\n",
        "\n",
        "if isColab:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "    student_colab_path = (\"/content/drive/MyDrive\"\n",
        "        + \"/Colab Notebooks\")\n",
        "    sys.path.append(student_colab_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrEKlNJIx6GR",
        "outputId": "8db60a13-f4ae-498b-ab82-f41644797e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WMf3nAPYyd0R"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Rescaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Aer8RX8SyNIx"
      },
      "outputs": [],
      "source": [
        "## COLAB SPECIFIC PATHS \n",
        "data_folder_path = \"/content/drive/MyDrive/Colab Notebooks/heridal_keras_retinanet_voc\"\n",
        "images_path = data_folder_path + \"/JPEGImages\"\n",
        "annotations_path = data_folder_path + \"/Annotations/JSON\"\n",
        "ImageSets = data_folder_path + \"/ImageSets/Main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "## GITHUB PATHS \n",
        "data_folder_path = \"data/heridal_keras_retinanet_voc\"\n",
        "images_path = data_folder_path + \"/JPEGImages\"\n",
        "annotations_path = data_folder_path + \"/Annotations/JSON\"\n",
        "ImageSets = data_folder_path + \"/ImageSets/Main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yCcLFUGJBLwN"
      },
      "outputs": [],
      "source": [
        "def xml_to_json(xml_folder, json_folder):\n",
        "    os.makedirs(json_folder, exist_ok=True)\n",
        "\n",
        "    for xml_file in os.listdir(xml_folder):\n",
        "        if xml_file.endswith('.xml'):\n",
        "            xml_path = os.path.join(xml_folder, xml_file)\n",
        "            json_path = os.path.join(json_folder, xml_file.replace('.xml', '.json'))\n",
        "\n",
        "            tree = ET.parse(xml_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            data_dict = {\n",
        "                'folder': root.find('folder').text if root.find('folder') is not None else 'Unknown',\n",
        "                'filename': root.find('filename').text if root.find('filename') is not None else 'Unknown',\n",
        "                'size': {\n",
        "                    'width': int(root.find('size/width').text) if root.find('size/width') is not None else 0,\n",
        "                    'height': int(root.find('size/height').text) if root.find('size/height') is not None else 0,\n",
        "                    'depth': int(root.find('size/depth').text) if root.find('size/depth') is not None else 0,\n",
        "                },\n",
        "                'objects': []\n",
        "            }\n",
        "\n",
        "            for obj in root.findall('object'):\n",
        "                bbox = obj.find('bndbox')\n",
        "                data_dict['objects'].append({\n",
        "                    'name': obj.find('name').text if obj.find('name') is not None else 'Unknown',\n",
        "                    'pose': obj.find('pose').text if obj.find('pose') is not None else 'Unspecified',\n",
        "                    'truncated': int(obj.find('truncated').text) if obj.find('truncated') is not None else 0,\n",
        "                    'difficult': int(obj.find('difficult').text) if obj.find('difficult') is not None else 0,\n",
        "                    'bbox': {\n",
        "                        'xmin': int(bbox.find('xmin').text) if bbox.find('xmin') is not None else 0,\n",
        "                        'ymin': int(bbox.find('ymin').text) if bbox.find('ymin') is not None else 0,\n",
        "                        'xmax': int(bbox.find('xmax').text) if bbox.find('xmax') is not None else 0,\n",
        "                        'ymax': int(bbox.find('ymax').text) if bbox.find('ymax') is not None else 0,\n",
        "                    }\n",
        "                })\n",
        "\n",
        "            with open(json_path, 'w') as json_file:\n",
        "                json.dump(data_dict, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mDer5edLs6AV"
      },
      "outputs": [],
      "source": [
        "#only run if json files don't exist yet\n",
        "xml_folder = data_folder_path + \"/Annotations\"\n",
        "json_folder = annotations_path\n",
        "xml_to_json(xml_folder, json_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A27j8oOBEvdW"
      },
      "outputs": [],
      "source": [
        "def parse_json(json_string):\n",
        "    data = json.loads(json_string.decode('utf-8'))\n",
        "    if 'objects' not in data or not data['objects']:\n",
        "        return np.zeros((0, 4), dtype=np.float32), np.int32(data['size']['width']), np.int32(data['size']['height'])\n",
        "    bounding_boxes = np.array([\n",
        "        [np.int32(obj['bbox']['xmin']), np.int32(obj['bbox']['ymin']),\n",
        "         np.int32(obj['bbox']['xmax']), np.int32(obj['bbox']['ymax'])]\n",
        "        for obj in data['objects'] if obj['name'] == 'person'\n",
        "    ], dtype=np.float32)\n",
        "    original_width = np.int32(data['size']['width'])\n",
        "    original_height = np.int32(data['size']['height'])\n",
        "    return bounding_boxes, original_width, original_height\n",
        "\n",
        "def load_json(annotation_path):\n",
        "    json_string = tf.io.read_file(annotation_path)\n",
        "    bounding_boxes, original_width, original_height = tf.numpy_function(\n",
        "        parse_json, [json_string], [tf.float32, tf.int32, tf.int32]\n",
        "    )\n",
        "    bounding_boxes.set_shape([None, 4])\n",
        "    original_width.set_shape([])\n",
        "    original_height.set_shape([])\n",
        "    return bounding_boxes, original_width, original_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iVqLCizzy4KA"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 256\n",
        "PATCH_SIZE = 16\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZjUWk6gr18BA"
      },
      "outputs": [],
      "source": [
        "def scale_bounding_boxes(bounding_boxes, original_width, original_height, new_width, new_height, pad_w, pad_h):\n",
        "  if tf.size(bounding_boxes) == 0:\n",
        "    return tf.zeros((0, 4), dtype=tf.float32)\n",
        "\n",
        "  original_width = tf.cast(original_width, tf.float32)\n",
        "  original_height = tf.cast(original_height, tf.float32)\n",
        "  new_width = tf.cast(new_width, tf.float32)\n",
        "  new_height = tf.cast(new_height, tf.float32)\n",
        "  pad_w = tf.cast(pad_w[0], tf.float32)\n",
        "  pad_h = tf.cast(pad_h[0], tf.float32)\n",
        "\n",
        "  scale_x = new_width / original_width\n",
        "  scale_y = new_height / original_height\n",
        "\n",
        "  xmin_scaled = bounding_boxes[:, 0] * scale_x + pad_w\n",
        "  ymin_scaled = bounding_boxes[:, 1] * scale_y + pad_h\n",
        "  xmax_scaled = bounding_boxes[:, 2] * scale_x + pad_w\n",
        "  ymax_scaled = bounding_boxes[:, 3] * scale_y + pad_h\n",
        "\n",
        "  bounding_boxes_scaled = tf.stack([xmin_scaled, ymin_scaled, xmax_scaled, ymax_scaled], axis=1)\n",
        "\n",
        "  return bounding_boxes_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yKetMqwSqZNs"
      },
      "outputs": [],
      "source": [
        "def compute_overlap_counts(bboxes, image_size, patch_size):\n",
        "    num_patches_side = image_size // patch_size\n",
        "    patch_indices = tf.range(0, image_size, patch_size)\n",
        "    start_indices = tf.cast(tf.stack(tf.meshgrid(patch_indices, patch_indices, indexing='ij'), axis=-1), tf.float32)\n",
        "    end_indices = start_indices + patch_size\n",
        "\n",
        "    overlap_counts = tf.zeros((num_patches_side, num_patches_side), dtype=tf.int32)\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        xmin, ymin, xmax, ymax = bbox[0], bbox[1], bbox[2], bbox[3]\n",
        "        overlaps_x = tf.logical_and(start_indices[..., 1] < xmax, start_indices[..., 1] + patch_size > xmin)\n",
        "        overlaps_y = tf.logical_and(start_indices[..., 0] < ymax, start_indices[..., 0] + patch_size > ymin)\n",
        "        overlaps = tf.logical_and(overlaps_x, overlaps_y)\n",
        "\n",
        "        overlap_counts += tf.cast(overlaps, tf.int32)\n",
        "    \n",
        "    return tf.expand_dims(overlap_counts, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uK3sn44E2t41"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, annotation_path):\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "  original_shape = tf.cast(tf.shape(image)[:2], tf.float32)\n",
        "  ratio = IMAGE_SIZE / tf.reduce_max(original_shape)\n",
        "  new_shape = tf.round(original_shape * ratio)\n",
        "\n",
        "  image = tf.image.resize(image, tf.cast(new_shape, tf.int32))\n",
        "\n",
        "  pad_w = (IMAGE_SIZE - new_shape[1]) / 2\n",
        "  pad_h = (IMAGE_SIZE - new_shape[0]) / 2\n",
        "  pad_w = [pad_w, IMAGE_SIZE - new_shape[1] - pad_w]\n",
        "  pad_h = [pad_h, IMAGE_SIZE - new_shape[0] - pad_h]\n",
        "\n",
        "  image = tf.pad(image, [[int(pad_h[0]), int(pad_h[1])], [int(pad_w[0]), int(pad_w[1])], [0, 0]], constant_values=0)\n",
        "  image = Rescaling(1./255)(image)\n",
        "\n",
        "  patches = tf.image.extract_patches(\n",
        "      images=tf.expand_dims(image, 0),\n",
        "      sizes=[1, PATCH_SIZE, PATCH_SIZE, 1],\n",
        "      strides=[1, PATCH_SIZE, PATCH_SIZE, 1],\n",
        "      rates=[1, 1, 1, 1],\n",
        "      padding='VALID'\n",
        "  )\n",
        "  patches = tf.reshape(patches, [-1, PATCH_SIZE * PATCH_SIZE * 3])\n",
        "\n",
        "  bounding_boxes, original_width, original_height = load_json(annotation_path)\n",
        "  bounding_boxes_scaled = scale_bounding_boxes(\n",
        "      bounding_boxes, original_width, original_height,\n",
        "      new_shape[1], new_shape[0], pad_w, pad_h\n",
        "  )\n",
        "\n",
        "  overlap_counts = compute_overlap_counts(bounding_boxes_scaled, IMAGE_SIZE, PATCH_SIZE)\n",
        "\n",
        "  return patches, overlap_counts, bounding_boxes_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "n2Ha09g-6HFy"
      },
      "outputs": [],
      "source": [
        "def file_exists(file_path):\n",
        "    return tf.io.gfile.exists(file_path.numpy().decode())\n",
        "\n",
        "def create_filtered_dataset(images_path, annotations_path, subset_prefix):\n",
        "    image_files = tf.data.Dataset.list_files(os.path.join(images_path, subset_prefix + '_*.jpg'))\n",
        "\n",
        "    def filter_func(image_file):\n",
        "        annotation_file = tf.strings.regex_replace(image_file, 'JPEGImages', 'Annotations')\n",
        "        annotation_file = tf.strings.regex_replace(annotation_file, '\\.jpg', '.xml')\n",
        "        return tf.py_function(file_exists, [annotation_file], Tout=tf.bool)\n",
        "\n",
        "    filtered_image_dataset = image_files.filter(filter_func)\n",
        "\n",
        "    def load_and_preprocess_image(image_file):\n",
        "        annotation_file = tf.strings.regex_replace(image_file, 'JPEGImages', 'Annotations/JSON')\n",
        "        annotation_file = tf.strings.regex_replace(annotation_file, '\\.jpg', '.json')\n",
        "        return preprocess_image(image_file, annotation_file)\n",
        "\n",
        "    dataset = filtered_image_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bt2L-eN-acb",
        "outputId": "8a25c7b4-d680-46f2-d5c3-2417f5952d98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-20 11:36:21.995090: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-20 11:36:21.995524: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train patches shape: (256, 768)\n",
            "Train bounding boxes: tf.Tensor(\n",
            "[[222.848    108.22401  226.62401  113.600006]\n",
            " [ 23.04     143.42401   26.560001 146.176   ]\n",
            " [170.11201  212.92801  173.184    217.53601 ]], shape=(3, 4), dtype=float32)\n",
            "Train overlap counts shape: (16, 16, 1)\n",
            "Train sample overlap counts: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-20 11:36:22.617993: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [101]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-20 11:36:22.618230: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [101]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test patches shape: (256, 768)\n",
            "Test bounding boxes: tf.Tensor(\n",
            "[[119.104004 140.09601  121.79201  145.728   ]\n",
            " [121.66401  141.632    125.12     147.58401 ]\n",
            " [125.184006 144.384    127.04001  147.45601 ]], shape=(3, 4), dtype=float32)\n",
            "Test overlap counts shape: (16, 16, 1)\n",
            "Test sample overlap counts: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = create_filtered_dataset(images_path, annotations_path, 'train')\n",
        "test_dataset = create_filtered_dataset(images_path, annotations_path, 'test')\n",
        "\n",
        "for patches, overlap_counts, bboxes in train_dataset.take(1):\n",
        "    print('Train patches shape:', patches.shape)\n",
        "    print('Train bounding boxes:', bboxes)\n",
        "    print('Train overlap counts shape:', overlap_counts.shape)\n",
        "    print('Train sample overlap counts:', overlap_counts.numpy().flatten()[:256])\n",
        "\n",
        "for patches, overlap_counts, bboxes in test_dataset.take(1):\n",
        "    print('Test patches shape:', patches.shape)\n",
        "    print('Test bounding boxes:', bboxes)\n",
        "    print('Test overlap counts shape:', overlap_counts.shape)\n",
        "    print('Test sample overlap counts:', overlap_counts.numpy().flatten()[:256])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "XaoP6ksmxPRu",
        "outputId": "3f995925-044c-4505-dbe5-f8fb672832ec"
      },
      "outputs": [],
      "source": [
        "def display_image_with_bboxes(image_patches, overlap_counts, bbox_data, num_patches_side):\n",
        "    patches_reshaped = tf.reshape(image_patches, [num_patches_side, num_patches_side, PATCH_SIZE, PATCH_SIZE, 3])\n",
        "    patches_transposed = tf.transpose(patches_reshaped, [0, 2, 1, 3, 4])\n",
        "    image_reconstructed = tf.reshape(patches_transposed, [IMAGE_SIZE, IMAGE_SIZE, 3])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.imshow(image_reconstructed)\n",
        "\n",
        "    for bbox in bbox_data:\n",
        "        y_min = bbox[1]\n",
        "        x_min = bbox[0]\n",
        "        y_max = bbox[3]\n",
        "        x_max = bbox[2]\n",
        "        print(f\"Bounding Box: [{x_min}, {y_min}, {x_max}, {y_max}]\")\n",
        "\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        rect = mpatches.Rectangle((x_min, y_min), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    for i in range(num_patches_side):\n",
        "        for j in range(num_patches_side):\n",
        "            if overlap_counts[i, j, 0] > 0:\n",
        "                patch_x = j * PATCH_SIZE\n",
        "                patch_y = i * PATCH_SIZE\n",
        "                rect = mpatches.Rectangle((patch_x, patch_y), PATCH_SIZE, PATCH_SIZE, linewidth=1, edgecolor='yellow', facecolor='none', linestyle='--')\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(patch_x + PATCH_SIZE / 2, patch_y + PATCH_SIZE / 2, str(overlap_counts[i, j, 0]),\n",
        "                        color='yellow', ha='center', va='center', fontsize=8)\n",
        "\n",
        "    plt.title(\"Reconstructed Image with Bounding Boxes and Highlight Patches\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for image_patches, overlap_counts, bbox_data in train_dataset.take(1):\n",
        "    num_patches_side = IMAGE_SIZE // PATCH_SIZE\n",
        "    display_image_with_bboxes(image_patches.numpy(), overlap_counts.numpy(), bbox_data.numpy(), num_patches_side)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Class\n",
        "The following code is for the Model class of the Transformer model. \n",
        "\n",
        "Some key notes about our dataset: \n",
        "- 7 classes \n",
        "- (256, 768) inputs\n",
        "\n",
        "Some key notes about other parameters: \n",
        "- Query: of shape (batch_sz, target_seq_length, feature_dim)\n",
        "- Value: of shape (batch_sz, source_seq_length, feature_dim)\n",
        "- Key: of shape (batch_sz, source_seq_length, feature_dim)\n",
        "\n",
        "Our architecture using the below classes goes as follows: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters/Stats\n",
        "Hyperparameters and stats for our model pre-compilation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Hyperparameters & general stats\n",
        "params = {\n",
        "    'batch_sz': 64, \n",
        "    'num_classes': 8,\n",
        "    'vocab_sz': 8,  \n",
        "    'num_att_heads': 1, \n",
        "    'key_dim': 10, \n",
        "    'query_dim': 10, \n",
        "    'value_dim': 10, \n",
        "    'output_sz': 8, \n",
        "    'window_sz': 256,\n",
        "    'num_features': 768, \n",
        "    'emb_sz': 132, \n",
        "    'num_layers': 2, \n",
        "    'learning_rate': 0.01, \n",
        "    'num_epochs': 20, \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: This takes like way too long to run - could anyone create tensors that have a batch dimension?\n",
        "\n",
        "all_cts = set()\n",
        "all_cts.add(0)\n",
        "img_patches_shape = set()\n",
        "overlap_cts_shape = set()\n",
        "bbox_data_shape = set() \n",
        "len_train = 0\n",
        "\n",
        "train_img_patches = tf.Variable(tf.random.normal((1546, 256, 768), dtype=tf.float32))\n",
        "train_overlap_counts = tf.Variable(tf.zeros((1546, 16, 16, 1), dtype=tf.int32))\n",
        "\n",
        "for image_patches, overlap_counts, bbox_data in train_dataset:\n",
        "    # train_img_patches[len_train].assign(image_patches)\n",
        "    # train_overlap_counts[len_train].assign(overlap_counts)\n",
        "    len_train += 1  \n",
        "    uq_ct = np.unique(overlap_counts.numpy())\n",
        "    uq_ct = uq_ct[1:] \n",
        "    img_patches_shape.add(image_patches.shape)\n",
        "    overlap_cts_shape.add(overlap_counts.shape)\n",
        "    bbox_data_shape.add(bbox_data.shape) ## Bbox shape is NOT uniform unfortunately \n",
        "\n",
        "    for el in uq_ct: \n",
        "        all_cts.add(el)\n",
        "\n",
        "print(\"This is the total number of classes: \", all_cts)\n",
        "print(\"This is the size of the train dataset: \", len_train)\n",
        "print(\"These are the img patches shape: \", img_patches_shape)\n",
        "print(\"These are overlap cts shape: \", overlap_cts_shape)\n",
        "print(\"These are bbox shape: \", bbox_data_shape)\n",
        "\n",
        "len_test = 0\n",
        "test_img_patches = tf.Variable(tf.random.normal((101, 256, 768), dtype=tf.float32))\n",
        "test_overlap_counts = tf.Variable(tf.zeros((101, 16, 16, 1), dtype=tf.int32))\n",
        "for test_image_patches, test_overlap_counts, test_bbox_data in test_dataset:\n",
        "    # test_img_patches[len_test].assign(test_image_patches)\n",
        "    # test_overlap_counts[len_test].assign(test_overlap_counts)\n",
        "    len_test += 1\n",
        "\n",
        "print(\"This is the size of the test dataset: \", len_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional Encoding \n",
        "This defines our positional encoding class. This will positionally encode our input tensor so that the Transformer has some understanding of how the pixels are distributed spatially. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the positional embedding shape:  (64, 256, 132)\n"
          ]
        }
      ],
      "source": [
        "## Taken from class - positional encoding method. \n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1) \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "## Taken from class - positional embedding class that will look a token's embedding vector and add the corresponding position vector \n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, emb_sz):\n",
        "    super().__init__()\n",
        "    self.emb_sz = emb_sz\n",
        "    # self.embedding = tf.keras.layers.Embedding(vocab_size, emb_sz, mask_zero=True) <- embedding for NLP-like tasks \n",
        "    self.embedding = tf.keras.layers.Dense(emb_sz)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=emb_sz)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.emb_sz, tf.float32)) # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "  \n",
        "## UNIT TEST: check sizes @ embedding \n",
        "test_emb = tf.zeros((params['batch_sz'], params['window_sz'], params['num_features']))\n",
        "embed_img = PositionalEmbedding(vocab_size=params['vocab_sz'], emb_sz=params['emb_sz'])\n",
        "embed = embed_img(test_emb)\n",
        "print(\"This is the positional embedding shape: \", embed.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention\n",
        "- BaseAttention: super class that other variations of Attention can inherit from. Essentially combines Attention + LayerNormalization into one class.\n",
        "- Other subclasses..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the shape post-self attention:  (64, 256, 132)\n"
          ]
        }
      ],
      "source": [
        "## Base Attention Class\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "## This class is just a basic implementation of self-attention \n",
        "class GlobalSelfAttention(BaseAttention): \n",
        "    def call(self, x): \n",
        "      attn_output, attn_scores = self.mha(query=x, key=x, value=x, return_attention_scores=True)\n",
        "      # Cache the attention scores for plotting later.\n",
        "      self.last_attn_scores = attn_scores\n",
        "      x = self.add([x, attn_output])\n",
        "      x = self.layernorm(x)\n",
        "      return x\n",
        "    \n",
        "## This class is just the masked implementation of self-attention\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "## This class connects the Encoder and Decoder: takes in context sequence for keys and values as opposed to the input sequence.  \n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    # Multi-headed attention \n",
        "    attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "  \n",
        "sample_sa = GlobalSelfAttention(num_heads=params['num_att_heads'], key_dim=params['key_dim'])\n",
        "att_on_emb = sample_sa(embed)\n",
        "print(\"This is the shape post-self attention: \", att_on_emb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FeedForward Layers\n",
        "- This class defines feedforward layers that produce our desired output after we apply attention to our input. This combines ADD + LayerNormalization in as well. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, emb_sz, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(emb_sz),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder Block\n",
        "There are two classes in this section: \n",
        "- Encoder Layer: one unit of the block. Attention -> LayerNorm -> FeedForward NN. \n",
        "- Encoder Block: we can have many stacked Encoder layers. All this does is create a positional embedding for the input, add it to the input, then pass that into a stack of Encoder layers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, emb_sz, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.self_attention = BaseAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=emb_sz,\n",
        "        dropout=dropout_rate)\n",
        "    self.ffn = FeedForward(emb_sz, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, emb_sz, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.emb_sz = emb_sz\n",
        "    self.num_layers = num_layers\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, emb_sz=emb_sz)\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(emb_sz=emb_sz,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Should result in shape of (batch_size, seq_len, emb_sz).\n",
        "    x = self.dropout(x)  # Add dropout.\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "    return x  # Should result in shape of (batch_size, seq_len, emb_sz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decoder Block \n",
        "There are two classes in this section: \n",
        "- Decoder Block: one unit of the block. Self attention -> Cross attention -> Feed forward network. \n",
        "- Decoder: stacked Decoder layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, emb_sz, num_heads, dff, dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    ## Basic self-attention (tutorials might use causal self attention, look into what that does)\n",
        "    self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=emb_sz, dropout=dropout_rate)\n",
        "    ## Encoder-decoder attention \n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=emb_sz,\n",
        "        dropout=dropout_rate)\n",
        "    self.ffn = FeedForward(emb_sz, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, emb_sz)`.\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, emb_sz, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.emb_sz = emb_sz\n",
        "    self.num_layers = num_layers\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, emb_sz=emb_sz)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [DecoderLayer(emb_sz=emb_sz, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, emb_sz)\n",
        "    x = self.dropout(x)\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "    return x # The shape of x is (batch_size, target_seq_len, emb_sz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Model\n",
        "This combines our Encoder and Decoder blocks into a coherent model. It should look something like the following: \n",
        "- Positional Encoding + Addition of Context => \n",
        "- => Encoder Block: \n",
        "    - Encoder Layer 1: \n",
        "        - Self Attention \n",
        "        - Feed Forward NN \n",
        "            - Dense 1 \n",
        "            - Dense 2 \n",
        "            - Dropout \n",
        "            - Add & LayerNormalization\n",
        "    - Encoder Layer 2: \n",
        "        - Self Attention \n",
        "        - Feed Forward NN \n",
        "            - Dense 1 \n",
        "            - Dense 2 \n",
        "            - Dropout \n",
        "            - Add & LayerNormalization \n",
        "    - etc ...\n",
        "- => Decoder Block: \n",
        "    - Decoder Layer 1: \n",
        "        - Causal Self Attention \n",
        "        - Cross Attention \n",
        "        - Feed Forward NN \n",
        "            - Dense 1\n",
        "            - Dense 2 \n",
        "            - Dropout \n",
        "            - Add & LayerNormalization\n",
        "    - etc ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, emb_sz, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.emb_context = tf.keras.layers.Dense(emb_sz)\n",
        "    self.emb_patches = tf.keras.layers.Dense(emb_sz)\n",
        "    \n",
        "    self.encoder = Encoder(num_layers=num_layers, emb_sz=emb_sz,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, emb_sz=emb_sz,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  ## FORWARD PASS\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the first argument.\n",
        "    patches, bbox_context = inputs  \n",
        "\n",
        "    # flatten & upsample the context, input: the following should return (batch_sz, 256, emb_sz)\n",
        "    bbox_context = tf.reshape(bbox_context, (params['batch_sz'], -1, 1))\n",
        "    bbox_context = self.emb_context(bbox_context)\n",
        "    patches = self.emb_patches(patches)\n",
        "    embedded = bbox_context + patches\n",
        "\n",
        "    # The following should return: (batch_sz, 256, emb_sz)\n",
        "    context = self.encoder(embedded) # ** (batch_size, context_len, emb_sz)\n",
        "\n",
        "    # The following should return: (batch_size, 256, emb_sz)\n",
        "    x = self.decoder(patches, context)  # ** (batch_size, target_len, emb_sz)\n",
        "\n",
        "    # The following should return: (batch_size, 256, 8)\n",
        "    logits = self.final_layer(x)  # ** (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    # nk - idrk what this following section does...\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics. b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits\n",
        "  \n",
        "  ## TRAIN STEP (1 per epoch)\n",
        "  def train(): \n",
        "    pass "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy and Loss Metrics\n",
        "Title. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "  mask = label != 0\n",
        "  match = match & mask\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and Fit the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-20 19:15:40.989616: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-20 19:15:40.989879: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 256, 8)\n",
            "(64, 256, 8)\n",
            "(64, 256, 8)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-20 19:15:49.162549: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Loop execution was cancelled.\n",
            "\t [[{{node while/LoopCond/_30}}]]\n",
            "2024-04-20 19:15:49.165635: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Loop execution was cancelled.\n",
            "\t [[{{node while/LoopCond/_30}}]]\n",
            "2024-04-20 19:15:49.192855: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Loop execution was cancelled.\n",
            "\t [[{{node while/LoopCond/_30}}]]\n",
            "2024-04-20 19:15:49.216771: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Loop execution was cancelled.\n",
            "\t [[{{node while/LoopCond/_30}}]]\n",
            "2024-04-20 19:15:49.257013: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Loop execution was cancelled.\n",
            "\t [[{{node while/LoopCond/_30}}]]\n",
            "2024-04-20 19:15:49.264675: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Loop execution was cancelled.\n",
            "\t [[{{node while/LoopCond/_30}}]]\n",
            "2024-04-20 19:15:49.273280: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Loop execution was cancelled.\n",
            "\t [[{{node while/LoopCond/_30}}]]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[162], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m             logits \u001b[38;5;241m=\u001b[39m transformer((patches, bbox_context))\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[162], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_dataset)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_number, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batched_train_dataset): \n\u001b[1;32m     33\u001b[0m     patches, bbox_context \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 34\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_context\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/engine/training.py:558\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    556\u001b[0m     layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[159], line 36\u001b[0m, in \u001b[0;36mTransformer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     33\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(embedded) \u001b[38;5;66;03m# ** (batch_size, context_len, emb_sz)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# The following should return: (batch_size, 256, emb_sz)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ** (batch_size, target_len, emb_sz)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# The following should return: (batch_size, 256, 8)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(x)  \u001b[38;5;66;03m# ** (batch_size, target_len, target_vocab_size)\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[153], line 17\u001b[0m, in \u001b[0;36mDecoder.call\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 17\u001b[0m   x  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlast_attn_scores\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[156], line 14\u001b[0m, in \u001b[0;36mDecoderLayer.call\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, context):\n\u001b[0;32m---> 14\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal_self_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention(x\u001b[38;5;241m=\u001b[39mx, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;66;03m# Cache the last attention scores for plotting later\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[148], line 22\u001b[0m, in \u001b[0;36mCausalSelfAttention.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m   attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m      \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_causal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd([x, attn_output])\n\u001b[1;32m     28\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(x)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/layers/attention/multi_head_attention.py:590\u001b[0m, in \u001b[0;36mMultiHeadAttention.call\u001b[0;34m(self, query, value, key, attention_mask, return_attention_scores, training, use_causal_mask)\u001b[0m\n\u001b[1;32m    579\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_attention_mask(\n\u001b[1;32m    580\u001b[0m     query,\n\u001b[1;32m    581\u001b[0m     value,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m     use_causal_mask\u001b[38;5;241m=\u001b[39muse_causal_mask,\n\u001b[1;32m    585\u001b[0m )\n\u001b[1;32m    587\u001b[0m \u001b[38;5;66;03m#   N = `num_attention_heads`\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m#   H = `size_per_head`\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# `query` = [B, T, N ,H]\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# `key` = [B, S, N, H]\u001b[39;00m\n\u001b[1;32m    593\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_dense(key)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/layers/core/einsum_dense.py:207\u001b[0m, in \u001b[0;36mEinsumDense.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 207\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m         ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/ops/special_math_ops.py:762\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meinsum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinalg.einsum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(equation, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    620\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Tensor contraction over specified indices and outer product.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m  Einsum allows defining Tensors by defining their element-wise computation.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m      - number of inputs or their shapes are inconsistent with `equation`.\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 762\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_einsum_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/ops/special_math_ops.py:1199\u001b[0m, in \u001b[0;36m_einsum_v2\u001b[0;34m(equation, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1197\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ellipsis_label:\n\u001b[1;32m   1198\u001b[0m     resolved_equation \u001b[38;5;241m=\u001b[39m resolved_equation\u001b[38;5;241m.\u001b[39mreplace(ellipsis_label, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1199\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_linalg_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolved_equation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;66;03m# Send fully specified shapes to opt_einsum, since it cannot handle unknown\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;66;03m# dimensions. For unknown dimensions, we guess that the dimension equals 1.\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;66;03m# Instead of creating Tensors or NumPy arrays with the specified shape,\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;66;03m# create a dummy `shaped` object with a `shape` property.\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m shaped \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshaped\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/ops/gen_linalg_ops.py:1082\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(inputs, equation, name)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   1081\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEinsum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mequation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## Instantiations of above objects\n",
        "transformer = Transformer(num_layers=params['num_layers'], \n",
        "                          emb_sz=params['emb_sz'],\n",
        "                          num_heads=params['num_att_heads'], \n",
        "                          dff=params['num_features'], \n",
        "                          input_vocab_size=params['vocab_sz'],\n",
        "                          target_vocab_size=params['vocab_sz'])\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
        "\n",
        "## Compile the model \n",
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])\n",
        "\n",
        "## Custom train model method  \n",
        "     # - notably can't just batch like below because bbox data is incorporated into the iterator, it isn't uniform size \n",
        "        #$ batched_train_dataset = train_dataset.batch(batch_size=params['batch_sz'], drop_remainder=False)\n",
        "    # - had to modify Sam's stuff a little bit so that the last bbox thing was out the picture\n",
        "\n",
        "def remove_bbox(c1, c2, c3):\n",
        "    return c1, c2\n",
        "batchable_train_data = train_dataset.map(remove_bbox)\n",
        "batched_train_dataset = batchable_train_data.batch(batch_size=params['batch_sz'], drop_remainder=False)\n",
        "batchable_test_data = test_dataset.map(remove_bbox)\n",
        "batched_test_dataset = batchable_test_data.batch(batch_size=params['batch_sz'], drop_remainder=False)\n",
        "\n",
        "def train(params, train_dataset): \n",
        "    for e in range(params['num_epochs']): \n",
        "        num_batches = len_train // params['batch_sz']\n",
        "        train_dataset.shuffle(buffer_size=3) # shuffle the dataset every epoch\n",
        "        for batch_number, batch in enumerate(batched_train_dataset): \n",
        "            patches, bbox_context = batch\n",
        "            logits = transformer((patches, bbox_context))\n",
        "\n",
        "train(params, train_dataset)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
