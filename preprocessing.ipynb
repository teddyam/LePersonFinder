{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LePersonFinder\n",
        "Description: looks for people. \n",
        "Notes: The following code is for colab specific stuff, but see the below for non-colab specific paths. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88ZgqwNVqfjE",
        "outputId": "b5553a87-ea72-4eb9-fb7a-68368c3628bb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "isColab = \"google.colab\" in sys.modules\n",
        "if isColab:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    student_colab_path = (\"/content/drive/MyDrive\"\n",
        "        + \"/Colab Notebooks\")\n",
        "    sys.path.append(student_colab_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrEKlNJIx6GR",
        "outputId": "8db60a13-f4ae-498b-ab82-f41644797e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "WMf3nAPYyd0R"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Rescaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Aer8RX8SyNIx"
      },
      "outputs": [],
      "source": [
        "## COLAB SPECIFIC PATHS \n",
        "data_folder_path = \"/content/drive/MyDrive/Colab Notebooks/heridal_keras_retinanet_voc\"\n",
        "images_path = data_folder_path + \"/JPEGImages\"\n",
        "annotations_path = data_folder_path + \"/Annotations/JSON\"\n",
        "ImageSets = data_folder_path + \"/ImageSets/Main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "## GITHUB PATHS \n",
        "data_folder_path = \"data/heridal_keras_retinanet_voc\"\n",
        "images_path = data_folder_path + \"/JPEGImages\"\n",
        "annotations_path = data_folder_path + \"/Annotations/JSON\"\n",
        "ImageSets = data_folder_path + \"/ImageSets/Main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "yCcLFUGJBLwN"
      },
      "outputs": [],
      "source": [
        "def xml_to_json(xml_folder, json_folder):\n",
        "    os.makedirs(json_folder, exist_ok=True)\n",
        "\n",
        "    for xml_file in os.listdir(xml_folder):\n",
        "        if xml_file.endswith('.xml'):\n",
        "            xml_path = os.path.join(xml_folder, xml_file)\n",
        "            json_path = os.path.join(json_folder, xml_file.replace('.xml', '.json'))\n",
        "\n",
        "            tree = ET.parse(xml_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            data_dict = {\n",
        "                'folder': root.find('folder').text if root.find('folder') is not None else 'Unknown',\n",
        "                'filename': root.find('filename').text if root.find('filename') is not None else 'Unknown',\n",
        "                'size': {\n",
        "                    'width': int(root.find('size/width').text) if root.find('size/width') is not None else 0,\n",
        "                    'height': int(root.find('size/height').text) if root.find('size/height') is not None else 0,\n",
        "                    'depth': int(root.find('size/depth').text) if root.find('size/depth') is not None else 0,\n",
        "                },\n",
        "                'objects': []\n",
        "            }\n",
        "\n",
        "            for obj in root.findall('object'):\n",
        "                bbox = obj.find('bndbox')\n",
        "                data_dict['objects'].append({\n",
        "                    'name': obj.find('name').text if obj.find('name') is not None else 'Unknown',\n",
        "                    'pose': obj.find('pose').text if obj.find('pose') is not None else 'Unspecified',\n",
        "                    'truncated': int(obj.find('truncated').text) if obj.find('truncated') is not None else 0,\n",
        "                    'difficult': int(obj.find('difficult').text) if obj.find('difficult') is not None else 0,\n",
        "                    'bbox': {\n",
        "                        'xmin': int(bbox.find('xmin').text) if bbox.find('xmin') is not None else 0,\n",
        "                        'ymin': int(bbox.find('ymin').text) if bbox.find('ymin') is not None else 0,\n",
        "                        'xmax': int(bbox.find('xmax').text) if bbox.find('xmax') is not None else 0,\n",
        "                        'ymax': int(bbox.find('ymax').text) if bbox.find('ymax') is not None else 0,\n",
        "                    }\n",
        "                })\n",
        "\n",
        "            with open(json_path, 'w') as json_file:\n",
        "                json.dump(data_dict, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "mDer5edLs6AV"
      },
      "outputs": [],
      "source": [
        "#only run if json files don't exist yet\n",
        "xml_folder = data_folder_path + \"/Annotations\"\n",
        "json_folder = annotations_path\n",
        "xml_to_json(xml_folder, json_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "A27j8oOBEvdW"
      },
      "outputs": [],
      "source": [
        "def parse_json(json_string):\n",
        "    data = json.loads(json_string.decode('utf-8'))\n",
        "    if 'objects' not in data or not data['objects']:\n",
        "        return np.zeros((0, 4), dtype=np.float32), np.int32(0), np.int32(data['size']['width']), np.int32(data['size']['height'])\n",
        "    bounding_boxes = np.array([\n",
        "        [np.int32(obj['bbox']['xmin']), np.int32(obj['bbox']['ymin']),\n",
        "         np.int32(obj['bbox']['xmax']), np.int32(obj['bbox']['ymax'])]\n",
        "        for obj in data['objects'] if obj['name'] == 'person'\n",
        "    ], dtype=np.float32)\n",
        "    num_bboxes = np.int32(len(bounding_boxes)) \n",
        "    original_width = np.int32(data['size']['width'])\n",
        "    original_height = np.int32(data['size']['height'])\n",
        "    return bounding_boxes, num_bboxes, original_width, original_height\n",
        "\n",
        "def load_json(annotation_path):\n",
        "    json_string = tf.io.read_file(annotation_path)\n",
        "    bounding_boxes, num_bboxes, original_width, original_height = tf.numpy_function(\n",
        "        parse_json, [json_string], [tf.float32, tf.int32, tf.int32, tf.int32]\n",
        "    )\n",
        "    bounding_boxes.set_shape([None, 4])\n",
        "    num_bboxes.set_shape([])\n",
        "    original_width.set_shape([])\n",
        "    original_height.set_shape([])\n",
        "    return bounding_boxes, num_bboxes, original_width, original_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "iVqLCizzy4KA"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 256\n",
        "PATCH_SIZE = 16\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "PROJECTION_DIM = 64\n",
        "MAX_COUNT_BBOXES = 35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "ZjUWk6gr18BA"
      },
      "outputs": [],
      "source": [
        "def scale_bounding_boxes(bounding_boxes, original_width, original_height, new_width, new_height, pad_w, pad_h):\n",
        "  if tf.size(bounding_boxes) == 0:\n",
        "    return tf.zeros((0, 4), dtype=tf.float32)\n",
        "\n",
        "  original_width = tf.cast(original_width, tf.float32)\n",
        "  original_height = tf.cast(original_height, tf.float32)\n",
        "  new_width = tf.cast(new_width, tf.float32)\n",
        "  new_height = tf.cast(new_height, tf.float32)\n",
        "  pad_w = tf.cast(pad_w[0], tf.float32)\n",
        "  pad_h = tf.cast(pad_h[0], tf.float32)\n",
        "\n",
        "  scale_x = new_width / original_width\n",
        "  scale_y = new_height / original_height\n",
        "\n",
        "  xmin_scaled = bounding_boxes[:, 0] * scale_x + pad_w\n",
        "  ymin_scaled = bounding_boxes[:, 1] * scale_y + pad_h\n",
        "  xmax_scaled = bounding_boxes[:, 2] * scale_x + pad_w\n",
        "  ymax_scaled = bounding_boxes[:, 3] * scale_y + pad_h\n",
        "\n",
        "  bounding_boxes_scaled = tf.stack([xmin_scaled, ymin_scaled, xmax_scaled, ymax_scaled], axis=1)\n",
        "\n",
        "  return bounding_boxes_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "yKetMqwSqZNs"
      },
      "outputs": [],
      "source": [
        "def compute_overlap_counts(bboxes, image_size, patch_size):\n",
        "    num_patches_side = image_size // patch_size\n",
        "    patch_indices = tf.range(0, image_size, patch_size)\n",
        "    start_indices = tf.cast(tf.stack(tf.meshgrid(patch_indices, patch_indices, indexing='ij'), axis=-1), tf.float32)\n",
        "    end_indices = start_indices + patch_size\n",
        "\n",
        "    overlap_counts = tf.zeros((num_patches_side, num_patches_side), dtype=tf.int32)\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        xmin, ymin, xmax, ymax = bbox[0], bbox[1], bbox[2], bbox[3]\n",
        "        overlaps_x = tf.logical_and(start_indices[..., 1] < xmax, start_indices[..., 1] + patch_size > xmin)\n",
        "        overlaps_y = tf.logical_and(start_indices[..., 0] < ymax, start_indices[..., 0] + patch_size > ymin)\n",
        "        overlaps = tf.logical_and(overlaps_x, overlaps_y)\n",
        "\n",
        "        overlap_counts += tf.cast(overlaps, tf.int32)\n",
        "    \n",
        "    return tf.expand_dims(overlap_counts, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "uK3sn44E2t41"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, annotation_path):\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "  original_shape = tf.cast(tf.shape(image)[:2], tf.float32)\n",
        "  ratio = IMAGE_SIZE / tf.reduce_max(original_shape)\n",
        "  new_shape = tf.round(original_shape * ratio)\n",
        "\n",
        "  image = tf.image.resize(image, tf.cast(new_shape, tf.int32))\n",
        "\n",
        "  pad_w = (IMAGE_SIZE - new_shape[1]) / 2\n",
        "  pad_h = (IMAGE_SIZE - new_shape[0]) / 2\n",
        "  pad_w = [pad_w, IMAGE_SIZE - new_shape[1] - pad_w]\n",
        "  pad_h = [pad_h, IMAGE_SIZE - new_shape[0] - pad_h]\n",
        "\n",
        "  image = tf.pad(image, [[int(pad_h[0]), int(pad_h[1])], [int(pad_w[0]), int(pad_w[1])], [0, 0]], constant_values=0)\n",
        "  image = Rescaling(1./255)(image)\n",
        "\n",
        "  patches = tf.image.extract_patches(\n",
        "      images=tf.expand_dims(image, 0),\n",
        "      sizes=[1, PATCH_SIZE, PATCH_SIZE, 1],\n",
        "      strides=[1, PATCH_SIZE, PATCH_SIZE, 1],\n",
        "      rates=[1, 1, 1, 1],\n",
        "      padding='VALID'\n",
        "  )\n",
        "  patches = tf.reshape(patches, [-1, PATCH_SIZE * PATCH_SIZE * 3])\n",
        "\n",
        "  bounding_boxes, num_bboxes, original_width, original_height = load_json(annotation_path)\n",
        "  bounding_boxes_scaled = scale_bounding_boxes(\n",
        "      bounding_boxes, original_width, original_height,\n",
        "      new_shape[1], new_shape[0], pad_w, pad_h\n",
        "  )\n",
        "\n",
        "  overlap_counts = compute_overlap_counts(bounding_boxes_scaled, IMAGE_SIZE, PATCH_SIZE)\n",
        "  one_hot_bboxes = tf.one_hot(num_bboxes, MAX_COUNT_BBOXES)\n",
        "\n",
        "  return patches, overlap_counts, bounding_boxes_scaled, one_hot_bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "n2Ha09g-6HFy"
      },
      "outputs": [],
      "source": [
        "def file_exists(file_path):\n",
        "    return tf.io.gfile.exists(file_path.numpy().decode())\n",
        "\n",
        "def create_filtered_dataset(images_path, annotations_path, subset_prefix):\n",
        "    image_files = tf.data.Dataset.list_files(os.path.join(images_path, subset_prefix + '_*.jpg'))\n",
        "\n",
        "    def filter_func(image_file):\n",
        "        annotation_file = tf.strings.regex_replace(image_file, 'JPEGImages', 'Annotations')\n",
        "        annotation_file = tf.strings.regex_replace(annotation_file, '\\.jpg', '.xml')\n",
        "        return tf.py_function(file_exists, [annotation_file], Tout=tf.bool)\n",
        "\n",
        "    filtered_image_dataset = image_files.filter(filter_func)\n",
        "\n",
        "    def load_and_preprocess_image(image_file):\n",
        "        annotation_file = tf.strings.regex_replace(image_file, 'JPEGImages', 'Annotations/JSON')\n",
        "        annotation_file = tf.strings.regex_replace(annotation_file, '\\.jpg', '.json')\n",
        "        return preprocess_image(image_file, annotation_file)\n",
        "\n",
        "    dataset = filtered_image_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-26 13:40:16.131608: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-26 13:40:16.131852: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train patches shape: (256, 768)\n",
            "Train bounding boxes: tf.Tensor([], shape=(0, 4), dtype=float32)\n",
            "Train overlap counts shape: (16, 16, 1)\n",
            "Train sample overlap counts: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Train one-hot encoded bounding boxes: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-26 13:40:16.587537: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [101]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-26 13:40:16.587784: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [101]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test patches shape: (256, 768)\n",
            "Test bounding boxes: tf.Tensor(\n",
            "[[193.216     48.256    198.016     54.336   ]\n",
            " [202.36801   48.64     206.59201   53.760002]\n",
            " [205.88802   49.216003 208.576     53.888   ]\n",
            " [203.712     61.888    208.19202   66.304   ]\n",
            " [195.072     71.552    198.52802   75.392   ]\n",
            " [ 89.984     70.01601   93.824005  73.472   ]\n",
            " [ 87.296005  73.024     89.664     75.712006]\n",
            " [ 83.200005  70.464005  86.976006  73.984   ]\n",
            " [ 82.240005  66.048004  86.336006  70.01601 ]\n",
            " [140.48001  160.06401  147.712    168.192   ]\n",
            " [139.968    156.86401  144.70401  163.712   ]], shape=(11, 4), dtype=float32)\n",
            "Test overlap counts shape: (16, 16, 1)\n",
            "Test sample overlap counts: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 2 0 0 0 0 0 0 0 4 0 0 0 0\n",
            " 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Test one-hot encoded bounding boxes: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = create_filtered_dataset(images_path, annotations_path, 'train')\n",
        "test_dataset = create_filtered_dataset(images_path, annotations_path, 'test')\n",
        "\n",
        "for patches, overlap_counts, bboxes, one_hot_bboxes in train_dataset.take(1):\n",
        "    print('Train patches shape:', patches.shape)\n",
        "    print('Train bounding boxes:', bboxes)\n",
        "    print('Train overlap counts shape:', overlap_counts.shape)\n",
        "    print('Train sample overlap counts:', overlap_counts.numpy().flatten()[:256])\n",
        "    print('Train one-hot encoded bounding boxes:', one_hot_bboxes.numpy())\n",
        "\n",
        "for patches, overlap_counts, bboxes, one_hot_bboxes in test_dataset.take(1):\n",
        "    print('Test patches shape:', patches.shape)\n",
        "    print('Test bounding boxes:', bboxes)\n",
        "    print('Test overlap counts shape:', overlap_counts.shape)\n",
        "    print('Test sample overlap counts:', overlap_counts.numpy().flatten()[:256])\n",
        "    print('Test one-hot encoded bounding boxes:', one_hot_bboxes.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Class\n",
        "The following code is for the Model class of the Transformer model. \n",
        "\n",
        "Some key notes about our dataset: \n",
        "- 7 classes \n",
        "- (256, 768) inputs\n",
        "\n",
        "Some key notes about other parameters: \n",
        "- Query: of shape (batch_sz, target_seq_length, feature_dim)\n",
        "- Value: of shape (batch_sz, source_seq_length, feature_dim)\n",
        "- Key: of shape (batch_sz, source_seq_length, feature_dim)\n",
        "\n",
        "Our architecture using the below classes goes as follows: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters/Stats\n",
        "Hyperparameters and stats for our model pre-compilation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Hyperparameters & general stats\n",
        "params = {\n",
        "    'batch_sz': 64, \n",
        "    'num_classes': 35,\n",
        "    'vocab_sz': 35,  \n",
        "    'num_att_heads': 1, \n",
        "    'key_dim': 10, \n",
        "    'query_dim': 10, \n",
        "    'value_dim': 10, \n",
        "    'output_sz': 8, \n",
        "    'window_sz': 256,\n",
        "    'num_features': 768, \n",
        "    'emb_sz': 132, \n",
        "    'num_layers': 2, \n",
        "    'learning_rate': 0.01, \n",
        "    'num_epochs': 20, \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Don't run this next cell :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: This takes like way too long to run - could anyone create tensors that have a batch dimension?\n",
        "\n",
        "all_cts = set()\n",
        "all_cts.add(0)\n",
        "img_patches_shape = set()\n",
        "overlap_cts_shape = set()\n",
        "bbox_data_shape = set() \n",
        "len_train = 0\n",
        "\n",
        "train_img_patches = tf.Variable(tf.random.normal((1546, 256, 768), dtype=tf.float32))\n",
        "train_overlap_counts = tf.Variable(tf.zeros((1546, 16, 16, 1), dtype=tf.int32))\n",
        "\n",
        "for image_patches, overlap_counts, bbox_data in train_dataset:\n",
        "    len_train += 1  \n",
        "    img_patches_shape.add(image_patches.shape)\n",
        "    overlap_cts_shape.add(overlap_counts.shape)\n",
        "    bbox_data_shape.add(bbox_data.shape) ## Bbox shape is NOT uniform unfortunately \n",
        "    all_cts.add(bbox_data.shape[0])\n",
        "\n",
        "print(\"This is the size of the train dataset: \", len_train)\n",
        "print(\"These are the img patches shape: \", img_patches_shape)\n",
        "print(\"These are overlap cts shape: \", overlap_cts_shape)\n",
        "print(\"These are bbox shape: \", bbox_data_shape)\n",
        "\n",
        "len_test = 0\n",
        "test_img_patches = tf.Variable(tf.random.normal((101, 256, 768), dtype=tf.float32))\n",
        "test_overlap_counts = tf.Variable(tf.zeros((101, 16, 16, 1), dtype=tf.int32))\n",
        "for test_image_patches, test_overlap_counts, test_bbox_data in test_dataset:\n",
        "    all_cts.add(test_bbox_data.shape[0])\n",
        "    len_test += 1\n",
        "\n",
        "print(\"This is the size of the test dataset: \", len_test)\n",
        "print(\"This is the total number of classes: \", all_cts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional Encoding \n",
        "This defines our positional encoding class. This will positionally encode our input tensor so that the Transformer has some understanding of how the pixels are distributed spatially. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the positional embedding shape:  (64, 256, 132)\n"
          ]
        }
      ],
      "source": [
        "## Taken from class - positional encoding method. \n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1) \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "## Taken from class - positional embedding class that will look a token's embedding vector and add the corresponding position vector \n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, emb_sz):\n",
        "    super().__init__()\n",
        "    self.emb_sz = emb_sz\n",
        "    self.embedding = tf.keras.layers.Dense(emb_sz)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=emb_sz)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.emb_sz, tf.float32)) # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "  \n",
        "## UNIT TEST: check sizes @ embedding \n",
        "test_emb = tf.zeros((params['batch_sz'], params['window_sz'], params['num_features']))\n",
        "embed_img = PositionalEmbedding(vocab_size=params['vocab_sz'], emb_sz=params['emb_sz'])\n",
        "embed = embed_img(test_emb)\n",
        "print(\"This is the positional embedding shape: \", embed.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention\n",
        "- BaseAttention: super class that other variations of Attention can inherit from. Essentially combines Attention + LayerNormalization into one class.\n",
        "- Other subclasses..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the shape post-self attention:  (64, 256, 132)\n"
          ]
        }
      ],
      "source": [
        "## Base Attention Class\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "## This class is just a basic implementation of self-attention \n",
        "class GlobalSelfAttention(BaseAttention): \n",
        "    def call(self, x): \n",
        "      attn_output, attn_scores = self.mha(query=x, key=x, value=x, return_attention_scores=True)\n",
        "      # Cache the attention scores for plotting later.\n",
        "      self.last_attn_scores = attn_scores \n",
        "      x = self.add([x, attn_output])\n",
        "      x = self.layernorm(x)\n",
        "      return x\n",
        "    \n",
        "## This class is just the masked implementation of self-attention\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "## This class connects the Encoder and Decoder: takes in context sequence for keys and values as opposed to the input sequence.  \n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    # Multi-headed attention \n",
        "    attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "  \n",
        "sample_sa = GlobalSelfAttention(num_heads=params['num_att_heads'], key_dim=params['key_dim'])\n",
        "att_on_emb = sample_sa(embed)\n",
        "print(\"This is the shape post-self attention: \", att_on_emb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FeedForward Layers\n",
        "- This class defines feedforward layers that produce our desired output after we apply attention to our input. This combines ADD + LayerNormalization in as well. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, emb_sz, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(emb_sz),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder Block\n",
        "There are two classes in this section: \n",
        "- Encoder Layer: one unit of the block. Attention -> LayerNorm -> FeedForward NN. \n",
        "- Encoder Block: we can have many stacked Encoder layers. All this does is create a positional embedding for the input, add it to the input, then pass that into a stack of Encoder layers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, emb_sz, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.self_attention = BaseAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=emb_sz,\n",
        "        dropout=dropout_rate)\n",
        "    self.ffn = FeedForward(emb_sz, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, emb_sz, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.emb_sz = emb_sz\n",
        "    self.num_layers = num_layers\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, emb_sz=emb_sz)\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(emb_sz=emb_sz,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Should result in shape of (batch_size, seq_len, emb_sz).\n",
        "    x = self.dropout(x)  # Add dropout.\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "    return x  # Should result in shape of (batch_size, seq_len, emb_sz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decoder Block \n",
        "There are two classes in this section: \n",
        "- Decoder Block: one unit of the block. Self attention -> Cross attention -> Feed forward network. \n",
        "- Decoder: stacked Decoder layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, emb_sz, num_heads, dff, dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=emb_sz, dropout=dropout_rate) # Masked self attention \n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads, key_dim=emb_sz, dropout=dropout_rate)  # Encoder-decoder attention \n",
        "    self.ffn = FeedForward(emb_sz, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, emb_sz)`.\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, emb_sz, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1, all_patches):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.emb_sz = emb_sz\n",
        "    self.num_layers = num_layers\n",
        "    self.all_patches = all_patches\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, emb_sz=emb_sz)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [DecoderLayer(emb_sz=emb_sz, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
        "    self.ffout = tf.keras.layers.Dense(1)\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, emb_sz)\n",
        "    x = self.dropout(x)\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    if self.all_patches: \n",
        "      x = tf.transpose(x, perm=[0, 2, 1])\n",
        "      x = self.ffout(x)\n",
        "\n",
        "    return x # The shape of x is (batch_size, target_seq_len, emb_sz) (ideally) - if not (batch_size, 1, emb_sz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Model\n",
        "This combines our Encoder and Decoder blocks into a coherent model. It should look something like the following: \n",
        "- Positional Encoding + Addition of Context => \n",
        "- => Encoder Block: \n",
        "    - Encoder Layer 1: \n",
        "        - Self Attention \n",
        "        - Feed Forward NN \n",
        "            - Dense 1 \n",
        "            - Dense 2 \n",
        "            - Dropout \n",
        "            - Add & LayerNormalization\n",
        "    - Encoder Layer 2: \n",
        "        - Self Attention \n",
        "        - Feed Forward NN \n",
        "            - Dense 1 \n",
        "            - Dense 2 \n",
        "            - Dropout \n",
        "            - Add & LayerNormalization \n",
        "    - etc ...\n",
        "- => Decoder Block: \n",
        "    - Decoder Layer 1: \n",
        "        - Causal Self Attention \n",
        "        - Cross Attention \n",
        "        - Feed Forward NN \n",
        "            - Dense 1\n",
        "            - Dense 2 \n",
        "            - Dropout \n",
        "            - Add & LayerNormalization\n",
        "    - etc ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, emb_sz, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.emb_context = tf.keras.layers.Dense(emb_sz)\n",
        "    self.emb_patches = tf.keras.layers.Dense(emb_sz)\n",
        "    \n",
        "    self.encoder = Encoder(num_layers=num_layers, emb_sz=emb_sz,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, emb_sz=emb_sz,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate,\n",
        "                           all_patches=True)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  # COMPILE \n",
        "  def compile(self, optimizer, loss, metrics):\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = loss \n",
        "    self.accuracy_function = metrics[0]\n",
        "\n",
        "  # FORWARD PASS\n",
        "  def call(self, inputs):\n",
        "    patches, bbox_context = inputs  \n",
        "\n",
        "    # flatten & upsample the context, input: the following should return (batch_sz, 256, emb_sz)\n",
        "    bbox_context = tf.reshape(bbox_context, (bbox_context.shape[0], -1, 1))\n",
        "    bbox_context = self.emb_context(bbox_context)\n",
        "    patches = self.emb_patches(patches)\n",
        "    # embedded = bbox_context + patches\n",
        "    embedded = bbox_context\n",
        "\n",
        "    # The following should return: (batch_sz, 256, emb_sz): (batch_size, context_len, emb_sz)\n",
        "    context = self.encoder(embedded)\n",
        "    # The following should return: (batch_size, 256, emb_sz): (batch_size, target_len, emb_sz)\n",
        "    x = self.decoder(patches, context)\n",
        "    # The following should return: (batch_size, 256, 8): (batch_size, target_len, target_vocab_size)\n",
        "    logits = tf.squeeze(x, axis=2) \n",
        "    logits = self.final_layer(logits)\n",
        "    smax = tf.nn.softmax(logits) # TODO: softmax over the last dimension \n",
        "\n",
        "    # nk - idrk what this following section does...\n",
        "    try:\n",
        "      del logits._keras_mask  # Drop the keras mask, so it doesn't scale the losses/metrics. b/250038731\n",
        "    except AttributeError:\n",
        "      pass\n",
        "    return smax # Return the final output and the attention weights.\n",
        "  \n",
        "  # One hot encode the labels - auxiliary function in case\n",
        "  def ohe_labels(self, labels, num_classes): \n",
        "      indices = labels[:, 0] # Only take the first entry -> [64, 1] vector\n",
        "      labels = tf.one_hot(indices, num_classes)\n",
        "      return labels \n",
        "\n",
        "  ## TRAIN STEP (per epoch)\n",
        "  def train(self, train_dataset, params, train_metrics_dict): \n",
        "      train_loss_per_epoch, train_accuracy_per_epoch = 0, 0\n",
        "      train_dataset.shuffle(buffer_size=3) # Shuffle the dataset every epoch\n",
        "      num_batches = 0\n",
        "      for patches, bbox_context, labels in train_dataset:\n",
        "          with tf.GradientTape() as tape: \n",
        "            out = self((patches, bbox_context))\n",
        "            loss = self.loss_function(labels, out)\n",
        "            accuracy = self.accuracy_function(labels, out) \n",
        "            train_loss_per_epoch+=loss.numpy()\n",
        "            train_accuracy_per_epoch+=accuracy.numpy()\n",
        "            num_batches += 1\n",
        "          grads = tape.gradient(loss, self.trainable_variables) # Compute the gradients \n",
        "          self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "      train_loss_per_epoch = train_loss_per_epoch / num_batches\n",
        "      train_accuracy_per_epoch = train_accuracy_per_epoch / num_batches\n",
        "      train_metrics_dict['Loss'].append(train_loss_per_epoch)\n",
        "      train_metrics_dict['Accuracy'].append(train_accuracy_per_epoch)\n",
        "\n",
        "  def test(): \n",
        "     pass "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy and Loss Metrics\n",
        "Title. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=False, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "  # ** Note: we probably do need to mask somehow? \n",
        "  # print(loss)\n",
        "  # mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  # loss *= mask\n",
        "  # loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return tf.reduce_sum(loss)/(label.shape[0])\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "    # Compute the index of the maximum prediction. This represents the predicted class.\n",
        "    pred_indices = tf.argmax(pred, axis=1)\n",
        "    # Similarly, find the index of the true class from the one-hot encoded labels.\n",
        "    true_indices = tf.argmax(label, axis=1)\n",
        "    # Compare predicted indices with true indices to see where they match.\n",
        "    correct_predictions = tf.equal(pred_indices, true_indices)\n",
        "    # Calculate the accuracy as the mean of the correct predictions\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and Fit the Model\n",
        "Some notable modifications I had to make: \n",
        "1. For purposes of actually being able to do multi-class classifications, had to manually OHE the labels during the training loop (not ideal, but it works). \n",
        "2. Had to zip two separate datasets. Again not ideal but it's the only way I could batch the data without going insane/doing manually and also getting the labels because otherwise the tuple returned from each iteration of the dataset would not have uniform shapes, so I couldn't use built-ins. \n",
        "\n",
        "\n",
        "Labels: \n",
        "at each iteration: \n",
        "    iamge_patches, overlap_cts, bbox \n",
        "        - none are batched\n",
        "        - either: \n",
        "            - manually batch \n",
        "            - iamge_patches, overlap_cts << new dataset >> \n",
        "    [13,4] -> 13 bboxes \n",
        "    [batch_sz, number_of_patches, number_of_classes]\n",
        "    [64, 256, 35] <- [64, 35] OR [64, 1, 35] -> [64, 35]\n",
        "        - [64, 35, 256] -> [64, 35, 1] -> [64, 35]\n",
        "    [64, 35] \n",
        "    number_of_classes <- 35 total bboxes for each image, max \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-26 14:15:08.718522: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-26 14:15:08.718816: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 2.5615762996673586 | Accuracy: 0.28512500047683714\n"
          ]
        }
      ],
      "source": [
        "## Instantiations of above objects\n",
        "transformer = Transformer(num_layers=params['num_layers'], \n",
        "                          emb_sz=params['emb_sz'],\n",
        "                          num_heads=params['num_att_heads'], \n",
        "                          dff=params['num_features'], \n",
        "                          input_vocab_size=params['vocab_sz'],\n",
        "                          target_vocab_size=params['vocab_sz'])\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
        "\n",
        "## Compile the model \n",
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])\n",
        "\n",
        "## First create batched training/testing data \n",
        "def reconstruct_datasets(train_dataset, test_dataset): \n",
        "    '''\n",
        "    This is a method that reconstructs/batches our datasets.\n",
        "    '''\n",
        "    def remove_non_uniform_bbox(c1, c2, c3, c4): \n",
        "        return c1, c2, c4\n",
        "    batchable_train_data = train_dataset.map(remove_non_uniform_bbox)\n",
        "    batched_train_dataset = batchable_train_data.batch(batch_size=params['batch_sz'], drop_remainder=False)\n",
        "    batchable_test_data = test_dataset.map(remove_non_uniform_bbox)\n",
        "    batched_test_dataset = batchable_test_data.batch(batch_size=params['batch_sz'], drop_remainder=False)\n",
        "    return batched_train_dataset, batched_test_dataset\n",
        " \n",
        " \n",
        "def train(params, batched_train_dataset, train_metrics_dict): \n",
        "    '''\n",
        "    This is a method that calls on our model's custom train method. \n",
        "    '''\n",
        "    training_start = time.time()\n",
        "    for e in range(params['num_epochs']): \n",
        "        transformer.train(batched_train_dataset, params, train_metrics_dict)\n",
        "        epoch_loss, epoch_acc = train_metrics_dict['Loss'][e], train_metrics_dict['Accuracy'][e]\n",
        "        print(f'Epoch: {e} | Loss: {epoch_loss} | Accuracy: {epoch_acc}')\n",
        "    training_end = time.time()\n",
        "    print(f'Training took: {training_end-training_start // 60} minutes')\n",
        "        \n",
        "batched_train_dataset, batched_test_dataset = reconstruct_datasets(train_dataset, test_dataset)\n",
        "train_metrics_dict={'Loss':[], 'Accuracy':[]}\n",
        "train(params, batched_train_dataset, train_metrics_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-26 13:27:20.555207: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-26 13:27:20.555433: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1583]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2024-04-26 13:27:20.652981: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: 1-th value returned by pyfunc_13 is int64, but expects int32\n",
            "\t [[{{node PyFunc}}]]\n",
            "2024-04-26 13:27:20.653434: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: 1-th value returned by pyfunc_13 is int64, but expects int32\n",
            "\t [[{{node PyFunc}}]]\n",
            "2024-04-26 13:27:20.655930: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: 1-th value returned by pyfunc_13 is int64, but expects int32\n",
            "\t [[{{node PyFunc}}]]\n",
            "2024-04-26 13:27:20.656504: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: 1-th value returned by pyfunc_13 is int64, but expects int32\n",
            "\t [[{{node PyFunc}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(256, 768) (16, 16, 1) (4, 4) (35,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-26 13:27:20.858119: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: 1-th value returned by pyfunc_13 is int64, but expects int32\n",
            "\t [[{{node PyFunc}}]]\n"
          ]
        }
      ],
      "source": [
        "# Unit testing dataset\n",
        "for el1, el2, el3, el4 in train_dataset.take(1): \n",
        "    print(el1.shape, el2.shape, el3.shape, el4.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
